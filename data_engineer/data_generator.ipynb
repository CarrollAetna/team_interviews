{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "breathing-mainland",
   "metadata": {},
   "source": [
    "# Q1: Basic File ETL (similar to Adobe structure)\n",
    "\n",
    "## Dataset with the following\n",
    "    - tsv format\n",
    "    - evar columns\n",
    "    - separate header file\n",
    "    - separate evar mapping file\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "proper-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make column names\n",
    "import urllib.request\n",
    "import random\n",
    "import json\n",
    "import names\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "word_url = \"https://www.mit.edu/~ecprice/wordlist.10000\"\n",
    "response = urllib.request.urlopen(word_url)\n",
    "long_txt = response.read().decode()\n",
    "words = long_txt.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "premier-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_date(current):\n",
    "    return current + datetime.timedelta(days=random.randrange(30), hours=random.randrange(12), minutes=random.randrange(60))\n",
    "\n",
    "startDate = datetime.datetime(2021, 1, 1,0,0)\n",
    "\n",
    "def make_data(header_file, data_file, mapping_file, n_rows=1000):\n",
    "    columns = [\"date\", \"merge1\", \"merge2\", \"merge3\"]\n",
    "    mappings = {}\n",
    "    for i in range(1, 51):\n",
    "        columns.append(f\"evar{i}\")\n",
    "        coin_flip = random.choice([0, 1, 2])\n",
    "        if coin_flip == 1:\n",
    "            mappings[f\"evar{i}\"] = random.choice(words)\n",
    "\n",
    "    print(f\"{len(columns)} columns\")\n",
    "    print(f\"{len(list(mappings.items())) + 4} non-null columns\")\n",
    "\n",
    "    with open(f'{mapping_file}', 'w') as fp:\n",
    "        json.dump(mappings, fp)\n",
    "\n",
    "    d = {}\n",
    "    for k, v in mappings.items():\n",
    "        coin_flip = random.choice([0, 1, 2, 3])\n",
    "        if coin_flip == 0:\n",
    "            # str\n",
    "            str_list = [random.choice(words) for i in range(0, n_rows)]\n",
    "            random.shuffle(str_list)\n",
    "            d[k] = str_list\n",
    "        elif coin_flip == 1:\n",
    "            # int\n",
    "            int_list = [i for i in range(0, n_rows)]\n",
    "            random.shuffle(int_list)\n",
    "            d[k] = int_list\n",
    "        elif coin_flip == 2:\n",
    "            # float\n",
    "            float_list = [i/3.14 for i in range(0, n_rows)]\n",
    "            random.shuffle(float_list)\n",
    "            d[k] = float_list\n",
    "        elif coin_flip == 3:\n",
    "            # bool\n",
    "            d[k] = [bool(random.getrandbits(1)) for i in range(0, n_rows)]\n",
    "\n",
    "    merge1_d = []\n",
    "    merge2_d = []\n",
    "    merge3_d = []\n",
    "    date_d = []\n",
    "    for i in range(0, 1000):\n",
    "        merge1_d.append(i)\n",
    "        merge2_d.append(names.get_full_name())\n",
    "        merge3_d.append(hex(int(i*1.61)))\n",
    "        date_d.append(random_date(startDate))\n",
    "    d['merge1'] = merge1_d\n",
    "    d['merge2'] = merge2_d\n",
    "    d['merge3'] = merge3_d\n",
    "    d['date'] = date_d\n",
    "\n",
    "    df = pd.DataFrame(d, columns=columns)\n",
    "    df.dropna(axis=1).info()\n",
    "\n",
    "    # Export columns\n",
    "    print(f\"Printing header file to {header_file}\")\n",
    "    pd.DataFrame(columns=columns).to_csv(f'{header_file}', index=False)\n",
    "    # Export data to tsv without header\n",
    "    print(f\"Printing data file to {data_file}\")\n",
    "    df.to_csv(f'{data_file}', sep=\"\\t\", header=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "informational-desert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 columns\n",
      "15 non-null columns\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 15 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    1000 non-null   datetime64[ns]\n",
      " 1   merge1  1000 non-null   int64         \n",
      " 2   merge2  1000 non-null   object        \n",
      " 3   merge3  1000 non-null   object        \n",
      " 4   evar3   1000 non-null   object        \n",
      " 5   evar5   1000 non-null   object        \n",
      " 6   evar6   1000 non-null   object        \n",
      " 7   evar9   1000 non-null   bool          \n",
      " 8   evar10  1000 non-null   object        \n",
      " 9   evar12  1000 non-null   bool          \n",
      " 10  evar22  1000 non-null   object        \n",
      " 11  evar34  1000 non-null   int64         \n",
      " 12  evar40  1000 non-null   bool          \n",
      " 13  evar41  1000 non-null   int64         \n",
      " 14  evar47  1000 non-null   object        \n",
      "dtypes: bool(3), datetime64[ns](1), int64(3), object(8)\n",
      "memory usage: 96.8+ KB\n",
      "Printing header file to q1/columns.csv\n",
      "Printing data file to q1/data.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>merge1</th>\n",
       "      <th>merge2</th>\n",
       "      <th>merge3</th>\n",
       "      <th>evar1</th>\n",
       "      <th>evar2</th>\n",
       "      <th>evar3</th>\n",
       "      <th>evar4</th>\n",
       "      <th>evar5</th>\n",
       "      <th>evar6</th>\n",
       "      <th>...</th>\n",
       "      <th>evar41</th>\n",
       "      <th>evar42</th>\n",
       "      <th>evar43</th>\n",
       "      <th>evar44</th>\n",
       "      <th>evar45</th>\n",
       "      <th>evar46</th>\n",
       "      <th>evar47</th>\n",
       "      <th>evar48</th>\n",
       "      <th>evar49</th>\n",
       "      <th>evar50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-21 03:53:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Shannon Heming</td>\n",
       "      <td>0x0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>completion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>guam</td>\n",
       "      <td>elder</td>\n",
       "      <td>...</td>\n",
       "      <td>298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>excess</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-04 01:59:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Patrick Chaffin</td>\n",
       "      <td>0x1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accidents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feeling</td>\n",
       "      <td>judy</td>\n",
       "      <td>...</td>\n",
       "      <td>368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pregnancy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-01 05:34:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Rose Benner</td>\n",
       "      <td>0x3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>viewed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>applications</td>\n",
       "      <td>potter</td>\n",
       "      <td>...</td>\n",
       "      <td>690</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tony</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-25 00:23:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Robert Hoover</td>\n",
       "      <td>0x4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nj</td>\n",
       "      <td>NaN</td>\n",
       "      <td>associations</td>\n",
       "      <td>intake</td>\n",
       "      <td>...</td>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>devoted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-27 10:26:00</td>\n",
       "      <td>4</td>\n",
       "      <td>Audrey Currier</td>\n",
       "      <td>0x6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>extraction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>manually</td>\n",
       "      <td>exciting</td>\n",
       "      <td>...</td>\n",
       "      <td>286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  merge1           merge2 merge3 evar1 evar2       evar3  \\\n",
       "0 2021-01-21 03:53:00       0   Shannon Heming    0x0   NaN   NaN  completion   \n",
       "1 2021-01-04 01:59:00       1  Patrick Chaffin    0x1   NaN   NaN   accidents   \n",
       "2 2021-01-01 05:34:00       2      Rose Benner    0x3   NaN   NaN      viewed   \n",
       "3 2021-01-25 00:23:00       3    Robert Hoover    0x4   NaN   NaN          nj   \n",
       "4 2021-01-27 10:26:00       4   Audrey Currier    0x6   NaN   NaN  extraction   \n",
       "\n",
       "  evar4         evar5     evar6  ... evar41 evar42  evar43 evar44 evar45  \\\n",
       "0   NaN          guam     elder  ...    298    NaN     NaN    NaN    NaN   \n",
       "1   NaN       feeling      judy  ...    368    NaN     NaN    NaN    NaN   \n",
       "2   NaN  applications    potter  ...    690    NaN     NaN    NaN    NaN   \n",
       "3   NaN  associations    intake  ...     78    NaN     NaN    NaN    NaN   \n",
       "4   NaN      manually  exciting  ...    286    NaN     NaN    NaN    NaN   \n",
       "\n",
       "   evar46     evar47 evar48 evar49 evar50  \n",
       "0     NaN     excess    NaN    NaN    NaN  \n",
       "1     NaN  pregnancy    NaN    NaN    NaN  \n",
       "2     NaN       tony    NaN    NaN    NaN  \n",
       "3     NaN    devoted    NaN    NaN    NaN  \n",
       "4     NaN         gc    NaN    NaN    NaN  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = make_data('q1/columns.csv', 'q1/data.tsv', 'q1/mappings.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-school",
   "metadata": {},
   "source": [
    "# Q2: File ETL with bad data\n",
    "\n",
    "Dataset with the following\n",
    "\n",
    "    - tsv file format\n",
    "    - header csv file missing a comma\n",
    "    - mismatched datatypes in certain columns\n",
    "    - mismatched rows (add extra data in certain rows, remove some tabs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "confirmed-detection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 columns\n",
      "18 non-null columns\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 18 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    1000 non-null   datetime64[ns]\n",
      " 1   merge1  1000 non-null   int64         \n",
      " 2   merge2  1000 non-null   object        \n",
      " 3   merge3  1000 non-null   object        \n",
      " 4   evar5   1000 non-null   bool          \n",
      " 5   evar6   1000 non-null   object        \n",
      " 6   evar7   1000 non-null   int64         \n",
      " 7   evar8   1000 non-null   float64       \n",
      " 8   evar11  1000 non-null   bool          \n",
      " 9   evar12  1000 non-null   bool          \n",
      " 10  evar15  1000 non-null   bool          \n",
      " 11  evar24  1000 non-null   object        \n",
      " 12  evar38  1000 non-null   bool          \n",
      " 13  evar39  1000 non-null   int64         \n",
      " 14  evar44  1000 non-null   bool          \n",
      " 15  evar48  1000 non-null   bool          \n",
      " 16  evar49  1000 non-null   float64       \n",
      " 17  evar50  1000 non-null   float64       \n",
      "dtypes: bool(7), datetime64[ns](1), float64(3), int64(3), object(4)\n",
      "memory usage: 92.9+ KB\n",
      "Printing header file to q2/columns.csv\n",
      "Printing data file to q2/data.tsv\n",
      "ar38,eva\n",
      "date,merge1,merge2,merge3,evar1,evar2,evar3,evar4,evar5,evar6,evar7,evar8,evar9,evar10,evar11,evar12,evar13,evar14,evar15,evar16,evar17,evar18,evar19,evar20,evar21,evar22,evar23,evar24,evar25,evar26,evar27,evar28,evar29,evar30,evar31,evar32,evar33,evar34,evar35,evar36,evar37,evar38evar39,evar40,evar41,evar42,evar43,evar44,evar45,evar46,evar47,evar48,evar49,evar50\n"
     ]
    }
   ],
   "source": [
    "make_data('q2/columns.csv', 'q2/data.tsv', 'q2/mappings.json')\n",
    "\n",
    "def findOccurrences(s, ch):\n",
    "    return [i for i, letter in enumerate(s) if letter == ch]\n",
    "\n",
    "with open('q2/columns.csv') as f:\n",
    "    data = f.read()\n",
    "\n",
    "comma_i = findOccurrences(data, ',')\n",
    "comma_to_delete = random.choice(comma_i)\n",
    "print(data[comma_to_delete-4:comma_to_delete+4])\n",
    "new_data = data[:comma_to_delete] + data[comma_to_delete+1:]\n",
    "\n",
    "# Removing comma\n",
    "with open('q2/columns.csv', 'w') as f:\n",
    "    f.write(new_data)\n",
    "\n",
    "with open('q2/columns.csv') as f:\n",
    "    data = f.read().replace('\\n', '')\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "raising-timothy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "with open('q2/data.tsv') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "random.shuffle(data)\n",
    "remove_tab = data[0]\n",
    "add_extra_data = data[1]\n",
    "remove_data = data[2]\n",
    "remove_newline = data[3]\n",
    "change_dtype = data[4]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "permanent-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tab\n",
    "tab_i = findOccurrences(remove_tab, '\\t')\n",
    "tab_to_delete = random.choice(tab_i)\n",
    "new_data_removed_tab = remove_tab[:tab_to_delete] + remove_tab[tab_to_delete+1:]\n",
    "data.append(new_data_removed_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "raising-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add extra data\n",
    "extra_data = add_extra_data.split('\\t')\n",
    "extra_data.insert(-1, \"extra data\")\n",
    "new_extra_data = '\\t'.join(extra_data)\n",
    "data.append(new_extra_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "circular-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_data\n",
    "data_removed = remove_data.split('\\t')\n",
    "element_to_remove = random.choice(data_removed)\n",
    "data_removed.remove(element_to_remove)\n",
    "new_remove_data = '\\t'.join(data_removed)\n",
    "data.append(new_remove_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "parliamentary-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove new line\n",
    "newline_removed = remove_newline.split('\\t')\n",
    "del newline_removed[-1]\n",
    "newline_removed_data = '\\t'.join(newline_removed)\n",
    "data.append(newline_removed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "aggressive-alexander",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change dtype\n",
    "dtype_changed = change_dtype.split('\\t')\n",
    "for i, x in enumerate(dtype_changed):\n",
    "    if not x:\n",
    "        dtype_changed[i] = \"nonnull\"  \n",
    "        break;\n",
    "dtype_changed_data = '\\t'.join(dtype_changed)\n",
    "data.append(dtype_changed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "finite-plane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "write_data = data[5:]\n",
    "random.shuffle(write_data)\n",
    "print(len(write_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "front-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('q2/data.tsv', 'w') as f:\n",
    "    for line in write_data:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-treasury",
   "metadata": {},
   "source": [
    "# Q3: Write Function to load multiple files incrementally\n",
    "\n",
    "- Multiple datasets\n",
    "- Output to single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data('q3/columns.csv', 'q3/data-01.tsv', 'q3/mappings.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
